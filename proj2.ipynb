{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.metrics import log_loss, confusion_matrix, plot_roc_curve\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn') # pretty matplotlib plots\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "# import dataframe_image as dfi\n",
    "# X columns are\n",
    "#  website_name | text\n",
    "\n",
    "# Y columns are\n",
    "# is_positive_sentiment\n",
    "\n",
    "#Paths\n",
    "\n",
    "DATA_DIR = os.path.join(\"data\", \"data_reviews\")\n",
    "X_TRAIN  = os.path.join(DATA_DIR, \"x_train.csv\")\n",
    "Y_TRAIN  = os.path.join(DATA_DIR, \"y_train.csv\")\n",
    "X_TEST   = os.path.join(DATA_DIR, \"x_test.csv\")\n",
    "\n",
    "plots_dir = \"figures\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UTILITIES:\n",
    "# Computes the accuracy of a model using the TP, TN, FP and FN values.\n",
    "# This is most easily used by passing in the calc_TP_TN_FP_FN() function\n",
    "def compute_accuracy(TP, TN, FP, FN):\n",
    "    return (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "def compute_TPR_TNR_PPV_NPV(TP, TN, FP, FN):\n",
    "    try:\n",
    "        TPR = TP / (TP + FN)\n",
    "    except:\n",
    "        TPR = 0\n",
    "    try:\n",
    "        TNR = TN / (TN + FP)\n",
    "    except:\n",
    "        TNR = 0\n",
    "    try:\n",
    "        PPV = TP / (TP + FP)\n",
    "    except:\n",
    "        PPV = 0\n",
    "    try:\n",
    "        NPV = TN / (TN + FN)\n",
    "    except:\n",
    "        NPV = 0\n",
    "    \n",
    "    # print(\"{} {:.3f}\\n{} {:.3f}\\n{} {:.3f}\\n{} {:.3f}\".format(\"True Positve Rate: \", TPR,\n",
    "    #                                           \"True Negative Rate:\", TNR,\n",
    "    #                                           \"Positive Predictive Rate:\", PPV,\n",
    "    #                                           \"Negative Predictive Rate:\", NPV))\n",
    "    return TPR, TNR, PPV, NPV\n",
    "\n",
    "def calc_TP_TN_FP_FN(ytrue_N, yhat_N):\n",
    "    '''\n",
    "    \n",
    "    Args\n",
    "    ----\n",
    "    ytrue_N : 1D array of floats\n",
    "        Each entry represents the binary value (0 or 1) of 'true' label of one example\n",
    "        One entry per example in current dataset\n",
    "    yhat_N : 1D array of floats\n",
    "        Each entry represents a predicted binary value (either 0 or 1).\n",
    "        One entry per example in current dataset.\n",
    "        Needs to be same size as ytrue_N.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    TP : int\n",
    "        Number of true positives\n",
    "    TN : int\n",
    "        Number of true negatives\n",
    "    FP : int\n",
    "        Number of false positives\n",
    "    FN : int\n",
    "        Number of false negatives\n",
    "    '''\n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    zipped = zip(ytrue_N, yhat_N)\n",
    "    for pair in zipped:\n",
    "        if pair[0] == 0:\n",
    "            if pair[1] == 0:\n",
    "                TN += 1\n",
    "            else:\n",
    "                FP += 1\n",
    "        else:\n",
    "            if pair[1] == 1:\n",
    "                TP += 1\n",
    "            else:\n",
    "                FN += 1\n",
    "\n",
    "    return TP, TN, FP, FN\n",
    "\n",
    "\n",
    "def calc_perf_metrics_for_threshold(ytrue_N, yproba1_N, thresh=0.5):\n",
    "    ''' Compute performance metrics for a given probabilistic classifier and threshold\n",
    "    Args\n",
    "    ----\n",
    "    ytrue_N : 1D array of floats\n",
    "        Each entry represents the binary value (0 or 1) of 'true' label of one example\n",
    "        One entry per example in current dataset\n",
    "    yproba1_N : 1D array of floats\n",
    "        Each entry represents a probability (between 0 and 1) that correct label is positive (1)\n",
    "        One entry per example in current dataset\n",
    "        Needs to be same size as ytrue_N\n",
    "    thresh : float\n",
    "        Scalar threshold for converting probabilities into hard decisions\n",
    "        Calls an example \"positive\" if yproba1 >= thresh\n",
    "        Default value reflects a majority-classification approach (class is the one that gets\n",
    "        highest probability)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    acc : accuracy of predictions\n",
    "    tpr : true positive rate of predictions\n",
    "    tnr : true negative rate of predictions\n",
    "    ppv : positive predictive value of predictions\n",
    "    npv : negative predictive value of predictions\n",
    "    '''\n",
    "    # First convert the probabilities into hard choices.\n",
    "    converted_probs = []\n",
    "    for prob in yproba1_N:\n",
    "        if prob >= thresh:\n",
    "            converted_probs.append(1)\n",
    "        else:\n",
    "            converted_probs.append(0)\n",
    "\n",
    "    scores             = calc_TP_TN_FP_FN(ytrue_N, converted_probs)\n",
    "    acc                = compute_accuracy(*scores)\n",
    "    error              = 1 - acc\n",
    "    l_loss             = log_loss(ytrue_N, yproba1_N)\n",
    "    tpr, tnr, ppv, npv = compute_TPR_TNR_PPV_NPV(*scores)\n",
    "    \n",
    "    return acc, error, l_loss, tpr, tnr, ppv, npv\n",
    "\n",
    "# You can use this function later to make printing results easier; don't change it.\n",
    "def metrics_to_dataframe(model_name, ytrue_N, yproba1_N, thresh=0.5):\n",
    "    ''' Pretty print perf. metrics for a given probabilistic classifier and threshold\n",
    "    '''\n",
    "    acc, error, l_loss,\\\n",
    "    tpr, tnr, ppv, npv = calc_perf_metrics_for_threshold(ytrue_N, yproba1_N, thresh)\n",
    "    round_val = 3\n",
    "    df = pd.DataFrame(\n",
    "        [\n",
    "            [model_name, \n",
    "            str(round(acc, round_val)), \n",
    "            str(round(error, round_val)), \n",
    "            str(round(l_loss, round_val)),\n",
    "            str(round(tpr, round_val)),\n",
    "            str(round(tnr, round_val)),\n",
    "            str(round(ppv, round_val)),\n",
    "            str(round(npv, round_val))]\n",
    "        ],\n",
    "        columns=[\"Model\", \"Accuracy\", \"Error\", \"Log Loss\", \"TPR\", \"TNR\", \"PPV\", \"NPV\"]\n",
    "        )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# You can use this function later to make printing results easier; don't change it.\n",
    "def print_perf_metrics_for_threshold(ytrue_N, yproba1_N, thresh=0.5):\n",
    "    ''' Pretty print perf. metrics for a given probabilistic classifier and threshold\n",
    "    '''\n",
    "    acc, error, l_loss,\\\n",
    "    tpr, tnr, ppv, npv = calc_perf_metrics_for_threshold(ytrue_N, yproba1_N, thresh)\n",
    "    \n",
    "    ## Pretty print the results\n",
    "    print(\"ACC: {:.3f}\".format(acc))\n",
    "    print(\"ERR: {:.3f}\".format(error))\n",
    "    print(\"L_L: {:.3f}\".format(l_loss))\n",
    "    print(\"TPR: {:.3f}\".format(tpr))\n",
    "    print(\"TNR: {:.3f}\".format(tnr))\n",
    "    print(\"PPV: {:.3f}\".format(ppv))\n",
    "    print(\"NPV: {:.3f}\".format(npv))\n",
    "\n",
    "def calc_confusion_matrix_for_threshold(ytrue_N, yproba1_N, thresh=0.5):\n",
    "    ''' Compute the confusion matrix for a given probabilistic classifier and threshold\n",
    "    \n",
    "    Args\n",
    "    ----\n",
    "    ytrue_N : 1D array of floats\n",
    "        Each entry represents the binary value (0 or 1) of 'true' label of one example\n",
    "        One entry per example in current dataset\n",
    "    yproba1_N : 1D array of floats\n",
    "        Each entry represents a probability (between 0 and 1) that correct label is positive (1)\n",
    "        One entry per example in current dataset\n",
    "        Needs to be same size as ytrue_N\n",
    "    thresh : float\n",
    "        Scalar threshold for converting probabilities into hard decisions\n",
    "        Calls an example \"positive\" if yproba1 >= thresh\n",
    "        Default value reflects a majority-classification approach (class is the one that gets\n",
    "        highest probability)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cm_df : Pandas DataFrame\n",
    "        Can be printed like print(cm_df) to easily display results\n",
    "    '''\n",
    "    cm = confusion_matrix(ytrue_N, yproba1_N >= thresh)\n",
    "    cm_df = pd.DataFrame(data=cm, columns=[0, 1], index=[0, 1])\n",
    "    cm_df.columns.name = 'Predicted'\n",
    "    cm_df.index.name = 'True'\n",
    "    return cm_df\n",
    "\n",
    "def concat_results_and_save(results, filename):\n",
    "    final_df = results[0]\n",
    "\n",
    "    for res in results[1:]:\n",
    "        final_df = pd.concat([final_df, res], ignore_index=True)\n",
    "    \n",
    "    final_df = final_df.sort_values(by=[\"Accuracy\"], ignore_index=True)\n",
    "    print(final_df)\n",
    "    formatted_results = final_df.style.background_gradient()\n",
    "    # dfi.export(formatted_results, os.path.join(plots_dir, filename)) \n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "x_train = pd.read_csv(X_TRAIN)\n",
    "y_train = pd.read_csv(Y_TRAIN)\n",
    "x_test  = pd.read_csv(X_TEST)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Some simple text processing to get started\n",
    "def process_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "x_train['text'] = x_train['text'].apply(lambda x: process_text(x))\n",
    "x_test['text'] = x_test['text'].apply(lambda x: process_text(x))\n",
    "\n",
    "x_train['sentiment'] = x_train['text'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "x_test['sentiment'] = x_test['text'].apply(lambda x: TextBlob(x).sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This whole block is for creating the Bag Of Words charts.\n",
    "# it is left commented out so we don't reproduce the graphs on subsequent full \n",
    "# runs\n",
    "\n",
    "\n",
    "# def vectorizer_test(x_train, y_train, ngram_ranges):\n",
    "#     results = {}\n",
    "\n",
    "#     for range in ngram_ranges:\n",
    "#         # Creating the vectorizer that changes the words into columns of numbers\n",
    "#         bigram_vectorizer = CountVectorizer(ngram_range=range,\n",
    "#                                             token_pattern=r'\\b\\w+\\b', min_df=1)\n",
    "\n",
    "#         # We use the entire corpus from train and test to fit the model\n",
    "#         corpus = pd.concat([x_train.text, x_test.text])\n",
    "#         # Fit the corpus to the vectorizer\n",
    "#         X_2 = bigram_vectorizer.fit(corpus)\n",
    "\n",
    "#         # Transform the big X datasets into the vectorized representations\n",
    "#         x_train_trans = bigram_vectorizer.transform(x_train.text).toarray()\n",
    "        \n",
    "#         logreg = LogisticRegression()\n",
    "\n",
    "#         logreg.fit(x_train_trans, y_train['is_positive_sentiment'])\n",
    "\n",
    "#         acc = logreg.score(x_train_trans, y_train['is_positive_sentiment'])\n",
    "#         results[str(range)] = [acc, x_train_trans.shape[1]]\n",
    "\n",
    "#     df = pd.DataFrame.from_dict(data=results, orient='index', columns=[\"acc\", \"n_features\"])\n",
    "#     acc_fig = px.bar(df, x=df.index, y=\"acc\",text_auto='.3%', labels={\"index\": \"N_gram Range\", \"acc\": \"Accuracy\"})\n",
    "#     acc_fig.write_image(\"figures/bag_of_words/accuracy_bar.png\")\n",
    "\n",
    "#     feat_fig = px.bar(df, x=df.index, y=\"n_features\", text_auto='.3s', labels={\"index\": \"N_gram Range\", \"n_features\": \"Number of Features\"})\n",
    "#     feat_fig.write_image(\"figures/bag_of_words/features_bar.png\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngram_ranges = [(1,1), (1,2), (1,3), (1,4), (1,5)]\n",
    "\n",
    "# vectorizer_test(x_train, y_train, ngram_ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the vectorizer that changes the words into columns of numbers\n",
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 3),\n",
    "                                    token_pattern=r'\\b\\w+\\b', min_df=1)\n",
    "\n",
    "# We use the entire corpus from train and test to fit the model\n",
    "corpus = pd.concat([x_train.text, x_test.text])\n",
    "# Fit the corpus to the vectorizer\n",
    "X_2 = bigram_vectorizer.fit(corpus)\n",
    "\n",
    "# Transform the big X datasets into the vectorized representations\n",
    "x_train_trans = bigram_vectorizer.transform(x_train.text).toarray()\n",
    "x_test_trans  = bigram_vectorizer.transform(x_test.text).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the important feature lists and weights assosiated with them\n",
    "\n",
    "feature_list = X_2.get_feature_names_out()\n",
    "\n",
    "important_features = []\n",
    "not_important_features = []\n",
    "sentiment_weights = []\n",
    "for i, feature in enumerate(feature_list):\n",
    "    blob = TextBlob(feature)\n",
    "    sentiment = blob.sentiment.polarity\n",
    "    if sentiment != 0:\n",
    "        important_features.append(i)\n",
    "        sentiment_weights.append(sentiment)\n",
    "    else:\n",
    "        not_important_features.append(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the features from a copy of the big Xs that have no sentimentality\n",
    "train_sentiments = np.delete(x_train_trans.copy(), not_important_features, 1)\n",
    "train_sentiments = train_sentiments.astype(float)\n",
    "test_sentiments  = np.delete(x_test_trans.copy(), not_important_features, 1)\n",
    "test_sentiments  = test_sentiments.astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def assign_weights(features):\n",
    "    for i, row in enumerate(features):\n",
    "        for j, col in enumerate(row):\n",
    "            if col != 0:\n",
    "                features[i][j] = sentiment_weights[j]\n",
    "\n",
    "\n",
    "def print_non_0s(features):\n",
    "    for i, row in enumerate(features):\n",
    "        for j, col in enumerate(row):\n",
    "            if col != 0:\n",
    "                print(\"row:\", i, \"col:\", j, \"weight:\", col)\n",
    "\n",
    "assign_weights(train_sentiments)\n",
    "assign_weights(test_sentiments)\n",
    "# print_non_0s(train_trans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add in the column for the sentiment of the entire sentence that we \n",
    "# created earlier\n",
    "train_sentiments = np.hstack((train_sentiments, x_train.sentiment.to_frame()))\n",
    "test_sentiments  = np.hstack((test_sentiments, x_test.sentiment.to_frame()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_trans = np.hstack((x_train_trans, train_sentiments))\n",
    "x_test_trans  = np.hstack((x_test_trans, test_sentiments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NAIVE Check using simple LogisticRegression solver\n",
    "\n",
    "# logreg = LogisticRegression()\n",
    "# logreg.fit(x_train_trans, y_train['is_positive_sentiment'])\n",
    "# acc = logreg.score(x_train_trans, y_train['is_positive_sentiment'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seee results\n",
    "# print(acc)\n",
    "\n",
    "# res_df = metrics_to_dataframe(\"Logistic Regression | Features Include Sentiment Weights\",\n",
    "#                       y_train.is_positive_sentiment,\n",
    "#                       logreg.predict_proba(x_train_trans)[:, 1])\n",
    "\n",
    "# concat_results_and_save([res_df], \"part2_results.png\")\n",
    "\n",
    "# test_pred  = logreg.predict_proba(x_test_trans)[:, 1]\n",
    "# np.savetxt(\"yproba1_test.txt\", test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOGISTIC REGRESSION STUFFSSS\n",
    "# Using L2 as regularization penalty\n",
    "\n",
    "# model = LogisticRegression()\n",
    "# solvers = ['lbfgs', 'liblinear', 'sag', 'saga']\n",
    "# penalty = ['l2']\n",
    "# c_values = [0.01, 0.1, 1.0, 10, 100]\n",
    "\n",
    "# # define grid search\n",
    "# grid = dict(solver=solvers,penalty=penalty,C=c_values)\n",
    "# cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=1, random_state=1)\n",
    "# grid_search = GridSearchCV(\n",
    "#     estimator=model, \n",
    "#     param_grid=grid, \n",
    "#     n_jobs=-1, \n",
    "#     cv=cv, \n",
    "#     scoring='accuracy',\n",
    "#     error_score=0, \n",
    "#     return_train_score=True)\n",
    "# grid_result = grid_search.fit(x_train_trans, y_train.is_positive_sentiment)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # summarize results\n",
    "# print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "# means  = grid_result.cv_results_['mean_test_score']\n",
    "# stds   = grid_result.cv_results_['std_test_score']\n",
    "# params = grid_result.cv_results_['params']\n",
    "# for mean, stdev, param in zip(means, stds, params):\n",
    "#     print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "# L2_grid_search_df = pd.DataFrame.from_dict(grid_result.cv_results_)\n",
    "# L2_grid_search_df.to_csv(\"result_csvs/L2_grid_search_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets try L1 as the reg penalty\n",
    "# L1_model   = LogisticRegression()\n",
    "# L1_solvers = ['liblinear', 'saga']\n",
    "# L1_penalty = ['l1']\n",
    "# c_values   = [0.01, 0.1, 1.0, 10, 100]\n",
    "\n",
    "# # define grid search\n",
    "# L1_grid = dict(solver=L1_solvers, penalty=L1_penalty, C=c_values)\n",
    "# L1_cv   = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# L1_grid_search = GridSearchCV(\n",
    "#     estimator=L1_model, \n",
    "#     param_grid=L1_grid, \n",
    "#     n_jobs=-1, \n",
    "#     cv=L1_cv, \n",
    "#     scoring='accuracy',\n",
    "#     error_score=0,\n",
    "#     return_train_score=True)\n",
    "# L1_grid_result = L1_grid_search.fit(x_train_trans, y_train.is_positive_sentiment)\n",
    "\n",
    "# # summarize results\n",
    "# print(\"Best: %f using %s\" % (L1_grid_result.best_score_, L1_grid_result.best_params_))\n",
    "# L1_means  = L1_grid_result.cv_results_['mean_test_score']\n",
    "# L1_stds   = L1_grid_result.cv_results_['std_test_score']\n",
    "# L1_params = L1_grid_result.cv_results_['params']\n",
    "# for mean, stdev, param in zip(L1_means, L1_stds, L1_params):\n",
    "#     print(\"%f (%f) with: %r\" % (mean, stdev, param))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize results\n",
    "# print(\"Best: %f using %s\" % (L1_grid_result.best_score_, L1_grid_result.best_params_))\n",
    "# L1_means  = L1_grid_result.cv_results_['mean_test_score']\n",
    "# L1_stds   = L1_grid_result.cv_results_['std_test_score']\n",
    "# L1_params = L1_grid_result.cv_results_['params']\n",
    "# for mean, stdev, param in zip(L1_means, L1_stds, L1_params):\n",
    "#     print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "\n",
    "# L1_grid_search_df = pd.DataFrame.from_dict(L1_grid_result.cv_results_)\n",
    "# L1_grid_search_df.to_csv(\"result_csvs/L1_grid_search_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets try MLP\n",
    "\n",
    "# Things to modify\n",
    "# * Alpha for sure\n",
    "# * Random state, since class\n",
    "# * Activation function \n",
    "\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "# mlp_model         = MLPClassifier()\n",
    "# mlp_solver        = ['lbfgs']\n",
    "# mlp_random_states = [1,2,3,4,5]\n",
    "# mlp_activation_functions = [\"identity\", \"logistic\", \"tanh\", \"relu\"]\n",
    "# alphas = np.logspace(-1, 1, 5)\n",
    "\n",
    "# # define grid search\n",
    "# mlp_grid = dict(\n",
    "#     solver=mlp_solver,\n",
    "#     random_state=mlp_random_states, \n",
    "#     activation=mlp_activation_functions, \n",
    "#     alpha=alphas, \n",
    "# )\n",
    "# mlp_cv          = RepeatedStratifiedKFold(n_splits=5, n_repeats=1, random_state=1)\n",
    "# mlp_grid_search = GridSearchCV(\n",
    "#     estimator=mlp_model, \n",
    "#     param_grid=mlp_grid, \n",
    "#     n_jobs=4, \n",
    "#     cv=mlp_cv, \n",
    "#     scoring='accuracy',\n",
    "#     error_score=0,\n",
    "#     return_train_score=True)\n",
    "# mlp_grid_result = mlp_grid_search.fit(x_train_trans, y_train.is_positive_sentiment)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize results\n",
    "# print(\"Best: %f using %s\" % (mlp_grid_result.best_score_, mlp_grid_result.best_params_))\n",
    "# mlp_means  = mlp_grid_result.cv_results_['mean_test_score']\n",
    "# mlp_stds   = mlp_grid_result.cv_results_['std_test_score']\n",
    "# mlp_params = mlp_grid_result.cv_results_['params']\n",
    "# for mean, stdev, param in zip(mlp_means, mlp_stds, mlp_params):\n",
    "#     print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "# mlp_grid_search_results_df = pd.DataFrame.from_dict(mlp_grid_search.cv_results_)\n",
    "# mlp_grid_search_results_df.to_csv(\"results_csvs/mlp_grid_search_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADA boooooost me\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# define the model with default hyperparameters\n",
    "ada_model = AdaBoostClassifier()\n",
    "# define the grid of values to search\n",
    "ada_grid = dict(\n",
    "    # Number of Trees. Number of weak learners\n",
    "    n_estimators    = [int(x) for x in np.logspace(1, 4, 5)],      # [10, 21, 46, 100, 215, 464, 1000, 2154, 4641, 10000]\n",
    "    # Depth of Tree. We can make the models used in the ensemble less weak (more skillful) by increasing the depth of the decision tree.\n",
    "    # base_estimators = [1, 2, 3, 4, 5], # NEVERMIND!!!! FUCK IT\n",
    "    # lower rates are better for more trees, larger for less.\n",
    "    learning_rate   = [float(x) for x in np.logspace(-3, 1, 5)]     # [0.001, 0.01, 0.1, 1.0, 10.0]\n",
    ")\n",
    "\n",
    "# define the evaluation procedure\n",
    "ada_cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=1, random_state=1)\n",
    "# define the grid search procedure\n",
    "ada_grid_search = GridSearchCV(\n",
    "    estimator=ada_model, \n",
    "    param_grid=ada_grid, \n",
    "    n_jobs=2, \n",
    "    cv=ada_cv, \n",
    "    scoring='accuracy',\n",
    "    error_score=0,\n",
    "    return_train_score=True\n",
    "    )\n",
    "# execute the grid search\n",
    "ada_grid_result = ada_grid_search.fit(x_train_trans, y_train.is_positive_sentiment)\n",
    "# summarize the best score and configuration\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best: %f using %s\" % (ada_grid_result.best_score_, ada_grid_result.best_params_))\n",
    "# summarize all scores that were evaluated\n",
    "ada_means = ada_grid_result.cv_results_['mean_test_score']\n",
    "ada_stds = ada_grid_result.cv_results_['std_test_score']\n",
    "ada_params = ada_grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(ada_means, ada_stds, ada_params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "ada_grid_results_df = pd.DataFrame.from_dict(ada_grid_result.cv_results_)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b048ae79f5d6d443f96a220b29781c480bd44b008d7f504c1cf29c1b2de4aac1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
